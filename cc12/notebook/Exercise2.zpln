{
  "paragraphs": [
    {
      "text": "%md\n\n# Exercise 2\n\nIn this exercise we use PySpark to build a binary classifier to classify a given tweet is about KPOP or other topics using a supervised machine learning technique, SVM.\n\nFor parts marked with **[CODE CHANGE REQUIRED]** you need to modify or complete the code before execution.\nFor parts without **[CODE CHANGE REQUIRED]** , you can just run the given code.\n\nThe task here is to build a classifier to differentiate the KPOP tweets or otherwise.\n\nFor example, the following tweet message falls into the category of Korean Pop because it seems talking about someone from korea \n```text\ncrazy cool jae s lee's pic of street singer reflected in raindrops tuesday on 2nd ave  \n```\nOn the other hand, the following tweet is not revelant to KPOP. \n```text\naccident closes jae valley rd drivers advised to avoid area seek alternate routes\n```\nTo achieve the goal, we need to develop a classifier, which is a supervised machine learning technique. In this example, we consider using Support Vector Machine (SVM) as the classifier algorithm. On the higher level, we need to \"train\" the model with some manually labelled data and perform some tests against the trained model. As part of the input requirement the SVM expect the input data to represented as a label (either yes or no, 1 or 0) accompanied by the feature vector. The feature vector is a vector of values which uniquely differentiate one entry from another ideally. In the machine learning context, features have to be fixed by the programmers. \n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Exercise 2</h1>\n<p>In this exercise we use PySpark to build a binary classifier to classify a given tweet is about KPOP or other topics using a supervised machine learning technique, SVM.</p>\n<p>For parts marked with <strong>[CODE CHANGE REQUIRED]</strong> you need to modify or complete the code before execution.<br />\nFor parts without <strong>[CODE CHANGE REQUIRED]</strong> , you can just run the given code.</p>\n<p>The task here is to build a classifier to differentiate the KPOP tweets or otherwise.</p>\n<p>For example, the following tweet message falls into the category of Korean Pop because it seems talking about someone from korea</p>\n<pre><code class=\"language-text\">crazy cool jae s lee's pic of street singer reflected in raindrops tuesday on 2nd ave  \n</code></pre>\n<p>On the other hand, the following tweet is not revelant to KPOP.</p>\n<pre><code class=\"language-text\">accident closes jae valley rd drivers advised to avoid area seek alternate routes\n</code></pre>\n<p>To achieve the goal, we need to develop a classifier, which is a supervised machine learning technique. In this example, we consider using Support Vector Machine (SVM) as the classifier algorithm. On the higher level, we need to &ldquo;train&rdquo; the model with some manually labelled data and perform some tests against the trained model. As part of the input requirement the SVM expect the input data to represented as a label (either yes or no, 1 or 0) accompanied by the feature vector. The feature vector is a vector of values which uniquely differentiate one entry from another ideally. In the machine learning context, features have to be fixed by the programmers.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651660_374639097",
      "id": "paragraph_1605178429551_2115660037",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:4328"
    },
    {
      "text": "%md\n## Uploading the data\n\n**[CODE CHANGE REQUIRED]** \nModify the following bash cell according to your environment and upload the data.\n\nIn case running the below taking too long thus Zeppelin killed it. e.g. \n\n```text\nParagraph received a SIGTERM\nExitValue: 143\n```\n\nYou may copy, paste and run the commands in a terminal (via ssh).\n\nHowever due to a bug with hadoop version 3.3.x, we still see the following warning, which is fine.\n\n```text\n2021-11-03 14:39:57,306 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n```",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Uploading the data</h2>\n<p><strong>[CODE CHANGE REQUIRED]</strong><br />\nModify the following bash cell according to your environment and upload the data.</p>\n<p>In case running the below taking too long thus Zeppelin killed it. e.g.</p>\n<pre><code class=\"language-text\">Paragraph received a SIGTERM\nExitValue: 143\n</code></pre>\n<p>You may copy, paste and run the commands in a terminal (via ssh).</p>\n<p>However due to a bug with hadoop version 3.3.x, we still see the following warning, which is fine.</p>\n<pre><code class=\"language-text\">2021-11-03 14:39:57,306 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651660_712309609",
      "id": "paragraph_1605180791304_1414349089",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4329"
    },
    {
      "text": "%sh\nexport PATH=$PATH:/home/ec2-user/hadoop/bin/\n\nnamenode=ip-172-31-86-18 # TODO:change me\n\nhdfs dfs -rm -r hdfs://$namenode:9000/lab12/ex2/\nhdfs dfs -mkdir -p hdfs://$namenode:9000/lab12/ex2/\nhdfs dfs -put /home/ec2-user/git/cohort_problems/cc12/data/ex2/label_data hdfs://$namenode:9000/lab12/ex2/\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T05:11:18+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "INCOMPLETE",
        "msg": [
          {
            "type": "TEXT",
            "data": "rm: `hdfs://ip-172-31-86-18:9000/lab12/ex2/': No such file or directory\n2021-11-03 23:39:30,023 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:30,197 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:30,365 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:31,155 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:31,329 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:31,425 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:31,556 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:32,226 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:32,597 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:32,640 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:32,705 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:33,491 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:33,716 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:34,376 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:34,451 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:34,886 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:35,080 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:35,172 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:35,233 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:35,565 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:35,979 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:37,384 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:37,637 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:39,373 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:39,631 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:39,793 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:41,268 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:42,016 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:42,769 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:42,857 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:43,395 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:44,128 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:44,197 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:44,962 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:45,529 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:46,110 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:46,301 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:46,344 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:47,625 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:47,672 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:47,740 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:50,838 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:52,984 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:54,204 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:55,287 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:55,840 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:57,840 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:39:58,308 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:40:01,576 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:40:02,229 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:40:09,222 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n2021-11-03 23:40:10,961 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n"
          },
          {
            "type": "TEXT",
            "data": "Paragraph received a SIGTERM\nExitValue: 143"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651660_180043538",
      "id": "paragraph_1605180805891_423222708",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4330"
    },
    {
      "text": "%md\n## Importing and Setup\n\n**[CODE CHANGE REQUIRED]**\n\nLet's import all the require libraries and set the hadoop file system name node IP.\n\nWe make use of `numpy` a python library for numeric computation,\nIf Python complains about `numpy not found`, go to terminal and run in all the data nodes that you have in the cluster\n\n```bash\n$ sudo pip3 install numpy sets\n```\n\nAlternatively, you may can also use flintrock to issue the above command to all the nodes in your cluster\n\n```bash\n$ flintrock run-command my_test_cluster 'sudo pip3 install sets numpy'\n```",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Importing and Setup</h2>\n<p><strong>[CODE CHANGE REQUIRED]</strong></p>\n<p>Let&rsquo;s import all the require libraries and set the hadoop file system name node IP.</p>\n<p>We make use of <code>numpy</code> a python library for numeric computation,<br />\nIf Python complains about <code>numpy not found</code>, go to terminal and run in all the data nodes that you have in the cluster</p>\n<pre><code class=\"language-bash\">$ sudo pip3 install numpy sets\n</code></pre>\n<p>Alternatively, you may can also use flintrock to issue the above command to all the nodes in your cluster</p>\n<pre><code class=\"language-bash\">$ flintrock run-command my_test_cluster 'sudo pip3 install sets numpy'\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651661_1155302908",
      "id": "paragraph_1605180403945_532175899",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4331"
    },
    {
      "text": "%pyspark\n\n\nimport re\nimport sets, math\nimport numpy # make sure numpy is installed on all datanodes using the command pip3 install numpy\n\nfrom pyspark.sql import SQLContext\nfrom pyspark.mllib import *\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.classification import SVMWithSGD\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\nsparkSession = SparkSession.builder.appName(\"SVM notebook\").getOrCreate()\nsc = sparkSession.sparkContext\n\nhdfs_nn = \"ip-172-31-86-18\" # TODO: fixme\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651661_1324611649",
      "id": "paragraph_1605179067285_567379070",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4332"
    },
    {
      "text": "%md\n\n## Loading the data\nWe load the data from the HDFS. The `.sample(False,0.1)` is to perform sampling on the input dataset. If you are to run it on a full cluster, feel free to remove the sampling\n\nThe first argument is boolean flag is called `withReplacement`. When it is `True`, it allows the same element to appear more than once. \nThe second argument is the fraction of elements in the sampled results. `0.1` means we expect 10% of the entire data set in the samples. You might set it to a lower ratio if it takes too long to run in t2.micro.\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Loading the data</h2>\n<p>We load the data from the HDFS. The <code>.sample(False,0.1)</code> is to perform sampling on the input dataset. If you are to run it on a full cluster, feel free to remove the sampling</p>\n<p>The first argument is boolean flag is called <code>withReplacement</code>. When it is <code>True</code>, it allows the same element to appear more than once.<br />\nThe second argument is the fraction of elements in the sampled results. <code>0.1</code> means we expect 10% of the entire data set in the samples. You might set it to a lower ratio if it takes too long to run in t2.micro.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_130751569",
      "id": "paragraph_1605232208177_523565523",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4333"
    },
    {
      "text": "%pyspark\ndef remove_punct(tweet):\n    return re.sub('[\\'\".,!#]','',tweet)\n\nposTXT = sc.textFile(\"hdfs://%s:9000/lab12/ex2/label_data/Kpop/*.txt\" % hdfs_nn).sample(False,0.1).map(remove_punct)\nnegTXT = sc.textFile(\"hdfs://%s:9000/lab12/ex2/label_data/othertweet/*.txt\" % hdfs_nn).sample(False,0.1).map(remove_punct)\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_652850788",
      "id": "paragraph_1605232417247_368212173",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4334"
    },
    {
      "text": "%md \n\n## Exercise 2.1 Build a language model using TFIDF\n\nIn Natural Language Procoessing, we often model a language using bags of words model. The idea is to represent text in terms of vectors.\n\nOne of the simple and effective method is to use Term-Frequency Inversed Document Frequency.\n\n$$\nTFIDF(w) = TF(w) * log(NDoc/DF(w))\n$$\n\nwhere *NDoc* is the number of documents.\n\n\n*  TF is actually the word count. For instance, consider the following text data.\n```text\napple smart phones made by apple\nandroid smart phones made by others\n```\nWe assume that each line is a document, hence there are two documents here.\n\n* The term frequency is\n```text\napple, 2\nandroid, 1\nphones, 2\nsmart, 2\nmade, 2\nby, 2\nothers, 1\n```\nThe term frequency is basically the word count, i.e. the number of occurances of a word across all document.\n\n* The document frequency is \n\n```text\napple, 1\nandroid, 1\nphones, 2\nsmart, 2\nmade, 2\nby, 2\nothers, 1\n```\n\nThe document frequency is the number of documents a word is mentioned.\n\n\n* IDF is is the total number of documents/records divided by the total number of the documents/records containing the words. We apply logarithmic to the quotient. The IDF for the above example is\n```text\napple, log(2/1)\nandroid, log(2/1)\nphones, log(2/2)\nsmart, log(2/2)\nmade, log(2/2)\nby, log(2/2)\nothers, log(2/1)\n```\nthat is\n```text\napple, 0.693\nandroid, 0.693\nphones, 0\nsmart, 0\nmade, 0\nby, 0\nothers, 0.693\n```\n\n* TF-IDF is obtained by multiplying the TF with the IDF.\n```text\napple, 1.386\nandroid, 0.693\nphones, 0\nsmart, 0\nmade, 0\nby, 0\nothers, 0.693\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 2.1 Build a language model using TFIDF</h2>\n<p>In Natural Language Procoessing, we often model a language using bags of words model. The idea is to represent text in terms of vectors.</p>\n<p>One of the simple and effective method is to use Term-Frequency Inversed Document Frequency.</p>\n<p>$$<br />\nTFIDF(w) = TF(w) * log(NDoc/DF(w))<br />\n$$</p>\n<p>where <em>NDoc</em> is the number of documents.</p>\n<ul>\n<li>TF is actually the word count. For instance, consider the following text data.</li>\n</ul>\n<pre><code class=\"language-text\">apple smart phones made by apple\nandroid smart phones made by others\n</code></pre>\n<p>We assume that each line is a document, hence there are two documents here.</p>\n<ul>\n<li>The term frequency is</li>\n</ul>\n<pre><code class=\"language-text\">apple, 2\nandroid, 1\nphones, 2\nsmart, 2\nmade, 2\nby, 2\nothers, 1\n</code></pre>\n<p>The term frequency is basically the word count, i.e. the number of occurances of a word across all document.</p>\n<ul>\n<li>The document frequency is</li>\n</ul>\n<pre><code class=\"language-text\">apple, 1\nandroid, 1\nphones, 2\nsmart, 2\nmade, 2\nby, 2\nothers, 1\n</code></pre>\n<p>The document frequency is the number of documents a word is mentioned.</p>\n<ul>\n<li>IDF is is the total number of documents/records divided by the total number of the documents/records containing the words. We apply logarithmic to the quotient. The IDF for the above example is</li>\n</ul>\n<pre><code class=\"language-text\">apple, log(2/1)\nandroid, log(2/1)\nphones, log(2/2)\nsmart, log(2/2)\nmade, log(2/2)\nby, log(2/2)\nothers, log(2/1)\n</code></pre>\n<p>that is</p>\n<pre><code class=\"language-text\">apple, 0.693\nandroid, 0.693\nphones, 0\nsmart, 0\nmade, 0\nby, 0\nothers, 0.693\n</code></pre>\n<ul>\n<li>TF-IDF is obtained by multiplying the TF with the IDF.</li>\n</ul>\n<pre><code class=\"language-text\">apple, 1.386\nandroid, 0.693\nphones, 0\nsmart, 0\nmade, 0\nby, 0\nothers, 0.693\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_2079144158",
      "id": "paragraph_1605232726270_1724353295",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4335"
    },
    {
      "text": "%md\n### Define `tf`\n**[CODE CHANGE REQUIRED]** \nComplete the following snippet to define `tf`\n\n\n<style>\n    div.hidecode + pre {display: none}\n</style>\n<script>\ndoclick=function(e) {\n    e.nextSibling.nextSibling.style.display=\"block\";\n}\n</script>\n\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n\n```text\ntf is the same as word count\n```",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Define <code>tf</code></h3>\n<p><strong>[CODE CHANGE REQUIRED]</strong><br />\nComplete the following snippet to define <code>tf</code></p>\n<style>\n    div.hidecode + pre {display: none}\n</style>\n<script>\ndoclick=function(e) {\n    e.nextSibling.nextSibling.style.display=\"block\";\n}\n</script>\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n<pre><code class=\"language-text\">tf is the same as word count\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_2032248966",
      "id": "paragraph_1605234231611_806747453",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4336"
    },
    {
      "text": "%pyspark\ndef tf(terms): \n    '''\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, tf_score)\n    '''\n    # TODO\n    return None\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_1712104143",
      "id": "paragraph_1605179483069_2099191288",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4337"
    },
    {
      "text": "%md\n\n### Sample answer \n\n\n\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n\n```python\ndef tf(terms): \n    '''\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, tf_score)\n    '''\n    # ANSWER\n    return terms.flatMap(lambda seq: map(lambda w:(w,1), seq)).reduceByKey(lambda x,y:x + y)\n\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Sample answer</h3>\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n<pre><code class=\"language-python\">def tf(terms): \n    '''\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, tf_score)\n    '''\n    # ANSWER\n    return terms.flatMap(lambda seq: map(lambda w:(w,1), seq)).reduceByKey(lambda x,y:x + y)\n\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_542416402",
      "id": "paragraph_1635988997031_1126584342",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4338"
    },
    {
      "text": "%md\n\n### Test Case for `tf`\n\nRun the following cell, you should see\n\n```\n[('apple', 2), ('by', 2), ('android', 1), ('smart', 2), ('made', 2), ('phones', 2), ('others', 1)]\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Test Case for <code>tf</code></h3>\n<p>Run the following cell, you should see</p>\n<pre><code>[('apple', 2), ('by', 2), ('android', 1), ('smart', 2), ('made', 2), ('phones', 2), ('others', 1)]\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_1791724702",
      "id": "paragraph_1605235266288_1311314818",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4339"
    },
    {
      "text": "%pyspark\n\n\ndef one_grams(s):\n    return s.split()\n\n\ntest_terms = [one_grams(\"apple smart phones made by apple\"), one_grams(\"android smart phones made by others\")]\ntest_tf = tf(sc.parallelize(test_terms))\ntest_tf.collect()",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[('apple', 2), ('smart', 2), ('phones', 2), ('made', 2), ('by', 2), ('android', 1), ('others', 1)]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_1937691576",
      "id": "paragraph_1605235280114_459821824",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4340"
    },
    {
      "text": "%md\n\n### Define `df`\n\n**[CODE CHANGE REQUIRED]** \n\nComplete the following snippet to define `df`\n\n\n<style>\n    div.hidecode + pre {display: none}\n</style>\n<script>\ndoclick=function(e) {\n    e.nextSibling.nextSibling.style.display=\"block\";\n}\n</script>\n\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n\n```text\ndf differs from tf with a little bit. Instead of outputting (word,1) for every word in a tweet directly, we should remove the duplicating words (within the same tweet) first.\n```",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Define <code>df</code></h3>\n<p><strong>[CODE CHANGE REQUIRED]</strong></p>\n<p>Complete the following snippet to define <code>df</code></p>\n<style>\n    div.hidecode + pre {display: none}\n</style>\n<script>\ndoclick=function(e) {\n    e.nextSibling.nextSibling.style.display=\"block\";\n}\n</script>\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n<pre><code class=\"language-text\">df differs from tf with a little bit. Instead of outputting (word,1) for every word in a tweet directly, we should remove the duplicating words (within the same tweet) first.\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_290953451",
      "id": "paragraph_1605234282079_479103327",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4341"
    },
    {
      "text": "%pyspark\ndef df(terms): \n    '''\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, df_score)\n    '''\n    # TODO\n    return None\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_1891490174",
      "id": "paragraph_1605179487492_435846692",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4342"
    },
    {
      "text": "%md\n\n### Test Case for `df`\n\nRun the following cell, you will see\n\n```\n[('apple', 1), ('by', 2), ('android', 1), ('smart', 2), ('made', 2), ('phones', 2), ('others', 1)]\n```\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Test Case for <code>df</code></h3>\n<p>Run the following cell, you will see</p>\n<pre><code>[('apple', 1), ('by', 2), ('android', 1), ('smart', 2), ('made', 2), ('phones', 2), ('others', 1)]\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_31622307",
      "id": "paragraph_1605235480613_427430793",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4343"
    },
    {
      "text": "%pyspark\ntest_terms = [one_grams(\"apple smart phones made by apple\"), one_grams(\"android smart phones made by others\")]\ntest_df = df(sc.parallelize(test_terms))\ntest_df.collect()",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[('made', 2), ('apple', 1), ('phones', 2), ('smart', 2), ('by', 2), ('android', 1), ('others', 1)]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_1408176989",
      "id": "paragraph_1605235477579_1943833912",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4344"
    },
    {
      "text": "%md\n### Sample answer\n\n\n\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n\n```python\ndef df(terms): \n    '''\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, df_score)\n    '''\n    # ANSWER\n    return terms.flatMap(lambda seq: list(set(map(lambda w:(w,1), seq)))).reduceByKey(lambda x,y:x + y)\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Sample answer</h3>\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n<pre><code class=\"language-python\">def df(terms): \n    '''\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, df_score)\n    '''\n    # ANSWER\n    return terms.flatMap(lambda seq: list(set(map(lambda w:(w,1), seq)))).reduceByKey(lambda x,y:x + y)\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_515023723",
      "id": "paragraph_1635989138618_1738479822",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4345"
    },
    {
      "text": "%md\n\n### Define `tfidf`\n\n**[CODE CHANGE REQUIRED]** \n\nComplete the following snippet to define `tfidf`\n\n\n<style>\n    div.hidecode + pre {display: none}\n</style>\n<script>\ndoclick=function(e) {\n    e.nextSibling.nextSibling.style.display=\"block\";\n}\n</script>\n\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n\n```text\nLet r be an RDD. r.count() returns the size of r.\nLet r1, r2 be RDDs of key-value pairs. r1.join(r2) joins two RDDs by keys.\n```",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Define <code>tfidf</code></h3>\n<p><strong>[CODE CHANGE REQUIRED]</strong></p>\n<p>Complete the following snippet to define <code>tfidf</code></p>\n<style>\n    div.hidecode + pre {display: none}\n</style>\n<script>\ndoclick=function(e) {\n    e.nextSibling.nextSibling.style.display=\"block\";\n}\n</script>\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n<pre><code class=\"language-text\">Let r be an RDD. r.count() returns the size of r.\nLet r1, r2 be RDDs of key-value pairs. r1.join(r2) joins two RDDs by keys.\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651662_1328247970",
      "id": "paragraph_1605234414509_1351659785",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4346"
    },
    {
      "text": "%pyspark\ndef tfidf(terms): \n    '''\n    input\n    terms:  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (words, tfidf_score) sorted by tfidf_score in descending order.\n    '''\n    # TODO\n    return None\n    \n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_1210043332",
      "id": "paragraph_1605179527860_2127881722",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4347"
    },
    {
      "text": "%md\n\n### Sample answer\n\n\n\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n\n```python\ndef tfidf(terms): \n    '''\n    input\n    terms:  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (words, tfidf_score) sorted by tfidf_score in descending order.\n    '''\n    # ANSWER\n    dCount = terms.count()\n    tfreq = tf(terms)\n    dfreq = df(terms)\n    return tfreq.join(dfreq).map(lambda p :(p[0], p[1][0] * math.log(dCount/p[1][1]))).sortBy( lambda p : - p[1])\n```\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Sample answer</h3>\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n<pre><code class=\"language-python\">def tfidf(terms): \n    '''\n    input\n    terms:  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (words, tfidf_score) sorted by tfidf_score in descending order.\n    '''\n    # ANSWER\n    dCount = terms.count()\n    tfreq = tf(terms)\n    dfreq = df(terms)\n    return tfreq.join(dfreq).map(lambda p :(p[0], p[1][0] * math.log(dCount/p[1][1]))).sortBy( lambda p : - p[1])\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_1869664126",
      "id": "paragraph_1635983055395_1918324490",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4348"
    },
    {
      "text": "%md\n\n### Test case for `tfidf`\n\nRun the following cell you will see\n\n```\n[('apple', 1.3862943611198906), ('android', 0.6931471805599453), ('others', 0.6931471805599453), ('by', 0.0), ('smart', 0.0), ('made', 0.0), ('phones', 0.0)]\n```\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Test case for <code>tfidf</code></h3>\n<p>Run the following cell you will see</p>\n<pre><code>[('apple', 1.3862943611198906), ('android', 0.6931471805599453), ('others', 0.6931471805599453), ('by', 0.0), ('smart', 0.0), ('made', 0.0), ('phones', 0.0)]\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_977090033",
      "id": "paragraph_1605235637913_949701721",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4349"
    },
    {
      "text": "%pyspark\ntest_terms = [one_grams(\"apple smart phones made by apple\"), one_grams(\"android smart phones made by others\")]\ntest_tfidf = tfidf(sc.parallelize(test_terms))\ntest_tfidf.collect()",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[('apple', 1.3862943611198906), ('android', 0.6931471805599453), ('others', 0.6931471805599453), ('smart', 0.0), ('phones', 0.0), ('made', 0.0), ('by', 0.0)]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_1017070745",
      "id": "paragraph_1605235636171_1140499598",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4350"
    },
    {
      "text": "%md\n\n## Exercise 2.2 Defining the Label points\n\nRecall that each label point is a decimal value (the label) with a vector. \n\n* For all positive tweets (KPop tweets) the label will be `1` and for all negative tweets we set `0` as the label. \n* For the vector parts, we build them using the tweet messages and the top 150 TFIDF\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 2.2 Defining the Label points</h2>\n<p>Recall that each label point is a decimal value (the label) with a vector.</p>\n<ul>\n<li>For all positive tweets (KPop tweets) the label will be <code>1</code> and for all negative tweets we set <code>0</code> as the label.</li>\n<li>For the vector parts, we build them using the tweet messages and the top 150 TFIDF</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_337902454",
      "id": "paragraph_1605235148663_387429256",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4351"
    },
    {
      "text": "%pyspark\n# You don't need to modify this cell\ndef buildTopTFIDF(tweets,tokenizer):\n    '''\n    input\n    tweets: an RDD of texts|\n    tokenizer: a function turns a string into list of tokens\n    \n    output\n    a list containing top 150 tfidf terms\n    '''\n    terms = tweets.map(tokenizer)\n    return map(lambda p:p[0], tfidf(terms).take(150))\n    ",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_1158223023",
      "id": "paragraph_1605235760088_2054597265",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4352"
    },
    {
      "text": "%md\n### Tokenizer\n**[CODE CHANGE REQUIRED]** \nWe've been using single word tokens for the test cases. However sometime using a multi-word tokenizer will help improving the performance by taking the neighboring word into account. \nDefine a `two_grams` tokenizer\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Tokenizer</h3>\n<p><strong>[CODE CHANGE REQUIRED]</strong><br />\nWe&rsquo;ve been using single word tokens for the test cases. However sometime using a multi-word tokenizer will help improving the performance by taking the neighboring word into account.<br />\nDefine a <code>two_grams</code> tokenizer</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_1746094799",
      "id": "paragraph_1605239020058_1392438533",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4353"
    },
    {
      "text": "%pyspark\nfrom functools import reduce\n\ndef two_grams(str):\n   '''\n    input\n     str : a string\n    output\n     a list of strings (each string contains two consecutive words seperated by space)\n   '''\n   return None # TODO: fixme \n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_1314720594",
      "id": "paragraph_1605239137806_1086427717",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4354"
    },
    {
      "text": "%md\n\n### Sample answer\n\n\n\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n\n```python\ndef to_ngrams(str, n):\n    words = str.split()\n    tokens = [words] * (n-1) # replicate the list of words for n-1 times\n    dropped = map(lambda p: p[0][p[1]:], zip(tokens, range(1,n)))\n    return reduce(lambda acc,ts:map(lambda p : p[0] + \" \" + p[1], zip(acc,ts)), dropped, words)\n\ndef two_grams(str):\n    return to_ngrams(str, 2)\n\n\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Sample answer</h3>\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n<pre><code class=\"language-python\">def to_ngrams(str, n):\n    words = str.split()\n    tokens = [words] * (n-1) # replicate the list of words for n-1 times\n    dropped = map(lambda p: p[0][p[1]:], zip(tokens, range(1,n)))\n    return reduce(lambda acc,ts:map(lambda p : p[0] + &quot; &quot; + p[1], zip(acc,ts)), dropped, words)\n\ndef two_grams(str):\n    return to_ngrams(str, 2)\n\n\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_416733055",
      "id": "paragraph_1635989599958_1932701258",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4355"
    },
    {
      "text": "%md\n\n### Test Case for `two_grams`\n\nRun the following you should see \n\n```text\n['The virus', 'virus that', 'that causes', 'causes COVID-19', 'COVID-19 is', 'is mainly', 'mainly transmitted', 'transmitted through', 'through droplets']\n```\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Test Case for <code>two_grams</code></h3>\n<p>Run the following you should see</p>\n<pre><code class=\"language-text\">['The virus', 'virus that', 'that causes', 'causes COVID-19', 'COVID-19 is', 'is mainly', 'mainly transmitted', 'transmitted through', 'through droplets']\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_175004775",
      "id": "paragraph_1605239394387_638254085",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4356"
    },
    {
      "text": "%pyspark\n\ns = \"The virus that causes COVID-19 is mainly  transmitted through droplets\"\nlist(two_grams(s))\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "['The virus', 'virus that', 'that causes', 'causes COVID-19', 'COVID-19 is', 'is mainly', 'mainly transmitted', 'transmitted through', 'through droplets']\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_1324440040",
      "id": "paragraph_1605239392461_1348676149",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4357"
    },
    {
      "text": "%md\n\nThe following cells build the top 150 TFIDF from the data that we loaded, you don't need to change anything. It might take a while to run (~ 25 mins on my t2.micro cluster)\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>The following cells build the top 150 TFIDF from the data that we loaded, you don&rsquo;t need to change anything. It might take a while to run (~ 25 mins on my t2.micro cluster)</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_429341571",
      "id": "paragraph_1605239765196_96339045",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4358"
    },
    {
      "text": "%pyspark\ntopTFIDF =  buildTopTFIDF(posTXT + negTXT,two_grams)",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 2: topTFIDF =  buildTopTFIDF(posTXT + negTXT,two_grams)\nTraceback (most recent call last):\n  File \"/tmp/1635982980699-0/zeppelin_python.py\", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"<stdin>\", line 13, in buildTopTFIDF\n  File \"<stdin>\", line 16, in tfidf\n  File \"/home/ec2-user/spark/python/pyspark/rdd.py\", line 812, in sortBy\n    return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()\n  File \"/home/ec2-user/spark/python/pyspark/rdd.py\", line 783, in sortByKey\n    samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()\n  File \"/home/ec2-user/spark/python/pyspark/rdd.py\", line 949, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/home/ec2-user/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1303, in __call__\n    answer = self.gateway_client.send_command(command)\n  File \"/home/ec2-user/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n    response = connection.send_command(command)\n  File \"/home/ec2-user/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1200, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n  File \"/usr/lib64/python3.7/socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\n  File \"/home/ec2-user/spark/python/pyspark/context.py\", line 285, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651663_648395107",
      "id": "paragraph_1605238979633_829412602",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4359"
    },
    {
      "text": "%pyspark\ntype(topTFIDF)",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 2: type(topTFIDF)\nTraceback (most recent call last):\n  File \"/tmp/1635982980699-0/zeppelin_python.py\", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\nNameError: name 'topTFIDF' is not defined\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651664_2014656597",
      "id": "paragraph_1605239727011_1423229505",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4360"
    },
    {
      "text": "%md \n\n## Defining `computeLP`\n**[CODE CHANGE REQUIRED]** \nComplete the following snippet.\n\nConcretely speaking, the `computeLP` function takes a label `1.0` or `0.0`, a sequence of string i.e. the 2-grams or 3-grams, and a array of top-N TF-IDF.\n\nFor each tf-idf term, let's say `t` is the i-th top-N TF-IDF term, if `t` is in the sequence of strings, we should put a `1.0` at the i-th position of the output vector, otherwise it should be `0.0`.\n\n<style>\n    div.hidecode + pre {display: none}\n</style>\n<script>\ndoclick=function(e) {\n    e.nextSibling.nextSibling.style.display=\"block\";\n}\n</script>\n\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n\n```text\nConvert all the words in the input text into a set instead of a list.\nThe output vector should be of the same dimension as topTerms (AKA top 150 TFIDF).\n```",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Defining <code>computeLP</code></h2>\n<p><strong>[CODE CHANGE REQUIRED]</strong><br />\nComplete the following snippet.</p>\n<p>Concretely speaking, the <code>computeLP</code> function takes a label <code>1.0</code> or <code>0.0</code>, a sequence of string i.e. the 2-grams or 3-grams, and a array of top-N TF-IDF.</p>\n<p>For each tf-idf term, let&rsquo;s say <code>t</code> is the i-th top-N TF-IDF term, if <code>t</code> is in the sequence of strings, we should put a <code>1.0</code> at the i-th position of the output vector, otherwise it should be <code>0.0</code>.</p>\n<style>\n    div.hidecode + pre {display: none}\n</style>\n<script>\ndoclick=function(e) {\n    e.nextSibling.nextSibling.style.display=\"block\";\n}\n</script>\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n<pre><code class=\"language-text\">Convert all the words in the input text into a set instead of a list.\nThe output vector should be of the same dimension as topTerms (AKA top 150 TFIDF).\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651664_999979015",
      "id": "paragraph_1605239798765_1070077038",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4361"
    },
    {
      "text": "%pyspark\n\n\ndef computeLP(label,text,tokenizer,topTerms):\n    '''\n    input\n    label : label 1 or 0\n    text : the text (String type)\n    tokenizer : the tokenizer\n    topTerms: the top TFIDF terms\n    \n    output:\n    a label point.\n    '''\n    seqSet = set(tokenizer(text))\n    scores = [0.0] * 150 # TODO: fixme\n    return LabeledPoint(label, Vectors.dense(scores))",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651664_1497086744",
      "id": "paragraph_1605235720192_1443219265",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4362"
    },
    {
      "text": "%md\n\n### Sample answer\n\n\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n\n\n```python\ndef computeLP(label,text,tokenizer,topTerms):\n    '''\n    input\n    label : label 1 or 0\n    text : the text (String type)\n    tokenizer : the tokenizer\n    topTerms: the top TFIDF terms\n    \n    output:\n    a label point.\n    '''\n    seqSet = set(tokenizer(text))\n    # ANSWER\n    scores = map(lambda t: 1.0 if t in seqSet else 0.0, list(topTerms))\n    return LabeledPoint(label, Vectors.dense(scores))\n````",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Sample answer</h3>\n<div class=\"hidecode\" onclick=\"doclick(this);\">[Show Hint]</div>\n<pre><code class=\"language-python\">def computeLP(label,text,tokenizer,topTerms):\n    '''\n    input\n    label : label 1 or 0\n    text : the text (String type)\n    tokenizer : the tokenizer\n    topTerms: the top TFIDF terms\n    \n    output:\n    a label point.\n    '''\n    seqSet = set(tokenizer(text))\n    # ANSWER\n    scores = map(lambda t: 1.0 if t in seqSet else 0.0, list(topTerms))\n    return LabeledPoint(label, Vectors.dense(scores))\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651664_2007138583",
      "id": "paragraph_1635989772361_1134484841",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4363"
    },
    {
      "text": "%md\n\n### Test Case for `computeLP`\n\nRun the following cell, you should see\n\n```\nLabeledPoint(1.0, [0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Test Case for <code>computeLP</code></h3>\n<p>Run the following cell, you should see</p>\n<pre><code>LabeledPoint(1.0, [0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651665_630896269",
      "id": "paragraph_1605240192287_1936336307",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4364"
    },
    {
      "text": "%pyspark\ncomputeLP(1.0, \"I love yoo jae suk\", two_grams, topTFIDF)",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 2: computeLP(1.0, \"I love yoo jae suk\", two_grams, topTFIDF)\nTraceback (most recent call last):\n  File \"/tmp/1635952418754-0/zeppelin_python.py\", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"<stdin>\", line 19, in computeLP\n  File \"/home/ec2-user/spark/python/pyspark/mllib/linalg/__init__.py\", line 914, in dense\n    return DenseVector(elements)\n  File \"/home/ec2-user/spark/python/pyspark/mllib/linalg/__init__.py\", line 283, in __init__\n    ar = np.array(ar, dtype=np.float64)\nTypeError: float() argument must be a string or a number, not 'map'\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651665_29028225",
      "id": "paragraph_1605240088165_1110133447",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4365"
    },
    {
      "text": "%md\n\n## Training the model\n\nLet's train our model. The codes are written for you, you don't need to change anything\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Training the model</h2>\n<p>Let&rsquo;s train our model. The codes are written for you, you don&rsquo;t need to change anything</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651665_1593335896",
      "id": "paragraph_1605240276004_1406005342",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4366"
    },
    {
      "text": "%pyspark\n\nposLP = posTXT.map( lambda twt: computeLP(1.0, twt, two_grams, topTFIDF) )\nnegLP = negTXT.map( lambda twt: computeLP(0.0, twt, two_grams, topTFIDF) )\n\ndata = negLP + posLP\n\n\n# Split data into training (60%) and test (40%).\n\nsplits = data.randomSplit([0.6,0.4],seed = 11L)\ntraining = splits[0].cache()\ntest = splits[1]\n\n# Run training algorithm to build the model\nnum_iteration = 100\nmodel = SVMWithSGD.train(training,num_iteration)\n\n# This will takes about 20 mins on a 4-core intel i7 processor 3.8GHZ with hyperthreading\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651665_1300161718",
      "id": "paragraph_1605179535041_2046301647",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4367"
    },
    {
      "text": "%md\n## Exercise 2.3 Evaluating the model\n\nWe apply the trained model to our testing data and evaluate the performance of our model. It should be around 84% accurate.\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Exercise 2.3 Evaluating the model</h2>\n<p>We apply the trained model to our testing data and evaluate the performance of our model. It should be around 84% accurate.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651665_416564984",
      "id": "paragraph_1605240382902_1712743203",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4368"
    },
    {
      "text": "\n%pyspark \nmodel.clearThreshold()\n# Compute raw scores on the test set\nscore_and_labels = test.map( lambda point: (float(model.predict(point.features)), point.label) )\n\n\n# Get the evaluation metrics\nmetrics = BinaryClassificationMetrics(score_and_labels)\nau_roc = metrics.areaUnderROC\n\nprint(\"Area under ROC = %s\" % str(au_roc))",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Area under ROC = 0.842790060653\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651665_1551177597",
      "id": "paragraph_1605240439551_792287451",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4369"
    },
    {
      "text": "%pyspark\nsc.stop()",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651665_1304280182",
      "id": "paragraph_1605240447746_694396597",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4370"
    },
    {
      "text": "%md\n\n## Cleaning up\n**[CODE CHANGE REQUIRED]** \nModify the following to clean up the HDFS\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Cleaning up</h2>\n<p><strong>[CODE CHANGE REQUIRED]</strong><br />\nModify the following to clean up the HDFS</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651665_1806062674",
      "id": "paragraph_1605245392089_949534435",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4371"
    },
    {
      "text": "%sh\nexport PATH=$PATH:/home/ec2-user/hadoop/bin/\n\nnamenode=ip-172-31-86-18 # TODO:change me\n\nhdfs dfs -rm -r hdfs://$namenode:9000/lab12/ex2/\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "20/11/13 13:39:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nDeleted /lab13/ex2\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651666_51095282",
      "id": "paragraph_1605245404306_140711229",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4372"
    },
    {
      "text": "%md\n\n# End of Exercise 2\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>End of Exercise 2</h1>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651666_1711240130",
      "id": "paragraph_1605245987387_984596966",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4373"
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-05T04:40:51+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1680669651666_151045885",
      "id": "paragraph_1635989963494_114454068",
      "dateCreated": "2023-04-05T04:40:51+0000",
      "status": "READY",
      "$$hashKey": "object:4374"
    }
  ],
  "name": "Exercise2",
  "id": "2HWKRZXQ1",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Exercise2"
}